---
i18nTitle: wiki.outputInterpretation.title
navKey: output-interpretation
---

# Interpreting results

The results produced by ValueApp are **decision-support information**, not promises, guarantees, or contractual truths.

They are meant to:
- support early discussions,
- create shared expectations,
- make assumptions explicit.

They are **not** meant to replace professional judgement.

---

## What the output represents

ValueApp outputs:
- an **effort range** (not a single number),
- broken down by **planning phase** and **service scope**.

This range reflects:
- uncertainty in early project stages,
- variability in leadership and coordination effort,
- differences in process stability.

![Effort range instead of a single value](wiki-assets/knowledge/output-effort-range.jpeg)

If you are looking for precision, you are likely looking too early.

---

## Why the result is a range

Planning effort cannot be calculated deterministically because:
- project requirements evolve,
- decisions are revisited,
- coordination intensity changes over time.

The range expresses:
- **lower bound**: optimistic, stable conditions,
- **upper bound**: higher coordination and iteration load.

The width of the range is as important as its absolute value.

A narrow range usually indicates:
- a well-defined project setup.

A wide range usually signals:
- unresolved organisational or procedural questions.

---

## Phase-based effort distribution

The output shows how effort is distributed across planning phases.

This helps to:
- understand **when** work happens,
- align staffing and cash flow expectations,
- compare projects structurally, not emotionally.

![Effort distributed across phases](wiki-assets/knowledge/output-phase-distribution.jpeg)

Important:
- Phase distributions are **model-based**, not schedules.
- They assume continuous progress, not stop-and-go reality.

---

## Comparing results between projects

Two projects can:
- look similar in size,
- feel similar in ambition,
- but produce different outputs.

Typical reasons include:
- different roles or service scopes,
- different organisational setups,
- different complexity or leadership factors.

![Similar projects, different outcomes](wiki-assets/knowledge/output-comparison-scenarios.jpeg)

Comparisons only make sense if:
- assumptions are aligned,
- roles and services are comparable,
- project maturity is similar.

---

## What the output does *not* say

The results do **not**:
- define fees,
- guarantee workload,
- assign responsibility,
- replace contractual agreements.

They also do **not**:
- validate underfunded projects,
- compensate for unclear scope,
- remove risk from planning.

If a number feels “wrong”, inspect the assumptions before questioning the model.

---

## Using the output responsibly

Good practice:
- communicate ranges, not just mid-values,
- document key assumptions alongside results,
- revisit calculations as the project evolves.

Avoid:
- fixing numbers too early,
- presenting results as binding commitments,
- hiding uncertainty to appear precise.

Precision is comforting. Transparency is useful.

---

## Where to go next

- [Quantity and baseline effort](quantity.html)
- [Complexity dimensions](complexity.html)
- [Adjustment (leadership) factor](adjustment-factor.html)
- [Roles and services](roles-and-services.html)
- [Workflows](workflows.html)
- [What ValueApp is – and what it is not](why-valueapp.html)